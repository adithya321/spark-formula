{% load_yaml as lookup_map %}
default:
  # used to control the names of SystemD services being created
  worker_service: 'spark-worker'
  master_service: 'spark-master'
  
  user: spark
  prefix: /usr/local/lib
  log_dir: /var/log/spark
  java_home: ~


  # change this to override the default SPARK_HOME/work 
  work_dir: ~
  pid_dir: /var/run/spark
  niceness: 0
  
  # controls the version of spark to be installed
  version: "2.3.0"
  # used to build the artifact filename
  hadoop_version: "2.7"
  # set to false if the "without-hadoop" version is desired
  with_hadoop: true
  # if without-hadoop == False, hadoop_home should also be set
  hadoop_home: ~
  # 
  archive_type: "tgz"

  # use a mirror to fetch the scala artifact
  archive_url_base: http://download.nextag.com/apache/spark
  # use the official servers to pull in Apache GPG keys and artifact signatures
  archive_hash_url_base: https://archive.apache.org/dist/spark

  # tells slaves where to look for a master
  master_host: 127.0.0.1
  master_port: 7077
  master_ui_port: 8080

  # params used when launching daemons
  # SPARK_WORKER_CORES - how many cores on the machine should spark consume. (Default: 0 is unlimited)
  worker_cores: 0

  # SPARK_WORKER_MEMORY - amount of heap allocated to each worker
  worker_memory: 2g

  # SPARK_WORKER_WEBUI_PORT
  worker_webui_port: 8081
  
  # SPARK_WORKER_OPTS - map of options to be set in the worker environments, flattened out to '-Dkey=val -Dfoo=bar'
  worker_opts: {}

  # SPARK_DAEMON_MEMORY - memory to allocate to the master/worker daemons
  daemon_memory: 1g

  # SPARK_DAEMON_JAVA_OPTS - list of options to be set in the master/worker environments
  daemon_java_opts: []
  # theese will be flattened out into -Dsomething=cool
  # daemon_java_opts:
  #   - something:cool

  # SPARK_PUBLIC_DNS - set this to override the machine's FQDN
  public_dns: ~
  

  init_system: ~
  init_scripts: ~
  init_overrides: ~

  package_deps: []

# Debian specific overrides to the above map
Debian:
  init_system: systemd
  init_scripts: /etc/systemd/system
  init_overrides: /etc/default
  package_deps:
    - scala
    - git
    - gpgv2
    - python-gnupg
  java_deps:
    - default-jre
  java_home: /usr/lib/jvm/java-8-openjdk-amd64/jre
  
# RHEL overrides
RedHat:
  init_system: systemd
  init_scripts: /usr/lib/systemd/system
  init_overrides: /etc/systemd/system
  package_deps:
    - scala
    - git
  java_deps:
    - java-1.8.0-openjdk
    - java-1.8.0-openjdk-devel
  java_home: /usr/lib/jvm/jre-1.8.0-openjdk.x86_64
{% endload %}


# These values are converted into java properties and written to /etc/spark/spark-defaults.conf
{% load_yaml as config %}
app:
  name: foobar

driver:
  cores: 1
  maxResultSize: 1g
  memory: 1g
  extraClassPath: ~
  extraJavaOptions: []
    
local:
  dir: /tmp/spark/scratch

executor:
  extraClassPath: ~
  extraJavaOptions: []
  extraLibraryPath: ~
  # names can be flattened too
  'logs.rolling.maxSize': ~
  
shuffle:
  compress: true
  'file.buffer': 32k
  'io.preferDirectBufs': true

ui:
  port: 4040
  retainedJobs: 1000
  retainedStages: 1000
  
{% endload %}
